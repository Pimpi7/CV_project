{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientNet implementation"
      ],
      "metadata": {
        "id": "9fkuIv5rQnyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sources\n"
      ],
      "metadata": {
        "id": "Liuv1xyod7D7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://docs.pytorch.org/vision/main/models/efficientnet.html \\\\\n",
        "https://github.com/lukemelas/EfficientNet-PyTorch \\\\\n",
        "https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py"
      ],
      "metadata": {
        "id": "fdf-4oHKZ_Vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Code"
      ],
      "metadata": {
        "id": "JJMEE30CeAYm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a7c38c6"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ],
      "metadata": {
        "id": "4Q7fPdF0eCLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from timm import create_model\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "%pip install wandb -q\n",
        "import wandb\n",
        "import random\n",
        "import math\n",
        "wandb.login()\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8zB6ghQeaBpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce307f4b-0c0a-468e-bc6a-bb3b1231c26d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Parameters"
      ],
      "metadata": {
        "id": "TiEsIXzdr3UK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Paths"
      ],
      "metadata": {
        "id": "FlhM_7y5zZzB"
      }
    },
    {
      "source": [
        "def merge_folders(folder1, folder2, folder3):\n",
        "  \"\"\"\n",
        "  Unisce il contenuto di due cartelle in una terza.\n",
        "\n",
        "  Args:\n",
        "      folder1: Percorso della prima cartella.\n",
        "      folder2: Percorso della seconda cartella.\n",
        "      folder3: Percorso della cartella di destinazione.\n",
        "  \"\"\"\n",
        "  # Crea la cartella di destinazione se non esiste\n",
        "  if not os.path.exists(folder3):\n",
        "    os.makedirs(folder3)\n",
        "\n",
        "  # Copia il contenuto di folder1 in folder3\n",
        "  for item in os.listdir(folder1):\n",
        "    source = os.path.join(folder1, item)\n",
        "    destination = os.path.join(folder3, item)\n",
        "    if os.path.isdir(source):\n",
        "      shutil.copytree(source, destination, dirs_exist_ok=True)\n",
        "    else:\n",
        "      shutil.copy2(source, destination)\n",
        "\n",
        "  # Copia il contenuto di folder2 in folder3\n",
        "  for item in os.listdir(folder2):\n",
        "    source = os.path.join(folder2, item)\n",
        "    destination = os.path.join(folder3, item)\n",
        "    if os.path.isdir(source):\n",
        "      shutil.copytree(source, destination, dirs_exist_ok=True)\n",
        "    else:\n",
        "      shutil.copy2(source, destination)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7EU5U18yV8ry"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Training set\n",
        "# t_fake = '/content/drive/MyDrive/CV_project/Dataset/Samples/train/fake'\n",
        "# t_real = '/content/drive/MyDrive/CV_project/Dataset/Samples/train/real'\n",
        "# t_all = '/content/drive/MyDrive/CV_project/Dataset/Samples/train/all'\n",
        "\n",
        "# #Test set\n",
        "# tst_fake = '/content/drive/MyDrive/CV_project/Dataset/Samples/test/fake'\n",
        "# tst_real = '/content/drive/MyDrive/CV_project/Dataset/Samples/test/real'\n",
        "# tst_all = '/content/drive/MyDrive/CV_project/Dataset/Samples/test/all'\n",
        "# #merge_folders(tst_fake, tst_real, tst_all)\n",
        "\n",
        "# #Validation set\n",
        "# v_fake = '/content/drive/MyDrive/CV_project/Dataset/Samples/validation/fake'\n",
        "# v_real = '/content/drive/MyDrive/CV_project/Dataset/Samples/validation/real'\n",
        "# v_all = '/content/drive/MyDrive/CV_project/Dataset/Samples/validation/all'\n",
        "# #merge_folders(v_fake, v_real, v_all)\n",
        "\n",
        "DATASET_PATHS = {\n",
        "    'train': '/content/drive/MyDrive/CV_project/Dataset/Samples/train',\n",
        "    'test': '/content/drive/MyDrive/CV_project/Dataset/Samples/test',\n",
        "    'eval': '/content/drive/MyDrive/CV_project/Dataset/Samples/validation'\n",
        "}"
      ],
      "metadata": {
        "id": "2BcrJK84r5Fb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def count_files(folder_path):\n",
        "#   \"\"\"Counts the number of files in a specified folder.\n",
        "\n",
        "#   Args:\n",
        "#     folder_path: The path to the folder.\n",
        "\n",
        "#   Returns:\n",
        "#     The number of files in the folder.\n",
        "#   \"\"\"\n",
        "#   num_files = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
        "#   return num_files\n",
        "\n",
        "# folder_path = DATASET_PATHS['train']\n",
        "# num_files_t = count_files(folder_path)\n",
        "# print(f\"Number of files in folder '{folder_path}': {num_files_t}\")\n",
        "\n",
        "# folder_path = DATASET_PATHS['test']\n",
        "# num_files_tst = count_files(folder_path)\n",
        "# print(f\"Number of files in folder '{folder_path}': {num_files_tst}\")\n",
        "\n",
        "# folder_path = DATASET_PATHS['eval']\n",
        "# num_files_v = count_files(folder_path)\n",
        "# print(f\"Number of files in folder '{folder_path}': {num_files_v}\")"
      ],
      "metadata": {
        "id": "cLmzEMlUaPYX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hyperparameters"
      ],
      "metadata": {
        "id": "MVVQwR-szdtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_CLASSES = 2  # Binaria per la deepfake detection\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "x7B073mMzfXR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Repeatibility"
      ],
      "metadata": {
        "id": "4TnmX6IYaAnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "KVZZa4TKaDxK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preprocessing"
      ],
      "metadata": {
        "id": "tyJRAZLNzL1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Caricamento dei dataset\n",
        "train_dataset = datasets.ImageFolder(DATASET_PATHS['train'], transform=transform)\n",
        "test_dataset = datasets.ImageFolder(DATASET_PATHS['test'], transform=transform)\n",
        "eval_dataset = datasets.ImageFolder(DATASET_PATHS['eval'], transform=transform)\n",
        "\n",
        "# Creazione dei DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "total_samples = len(train_dataset) + len(test_dataset) + len(eval_dataset)\n",
        "print(f\"Training samples: {len(train_dataset)}, Test samples: {len(test_dataset)}, Evaluation samples: {len(eval_dataset)}\")\n",
        "\n",
        "#Overwiev\n",
        "print(\"\\nOverview:\")\n",
        "print(f\"Total number of samples: {total_samples}\")\n",
        "print(f\"Split:\")\n",
        "print(f\"Training set: {round((len(train_dataset)*100)/total_samples)}%\")\n",
        "print(f\"Test set: {round((len(test_dataset)*100)/total_samples)}%\")\n",
        "print(f\"Validation set: {round((len(eval_dataset)*100)/total_samples)}%\")"
      ],
      "metadata": {
        "id": "TggX9yTyzP3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f8a77e7-4060-4d53-ca7c-8a6b101fec7d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 6998, Test samples: 1494, Evaluation samples: 1494\n",
            "\n",
            "Overview:\n",
            "Total number of samples: 9986\n",
            "Split:\n",
            "Training set: 70%\n",
            "Test set: 15%\n",
            "Validation set: 15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model (EfficientNet)"
      ],
      "metadata": {
        "id": "udGx4cm_iBy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model('efficientnetv2_rw_s', pretrained=False, num_classes=NUM_CLASSES)\n",
        "model = model.to(DEVICE)"
      ],
      "metadata": {
        "id": "dP1EIx_eiBUQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "JLHNus5Gzra8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "wandb.init(\n",
        "      entity = \"CV_Leo_Fra\"\n",
        "      project=\"EfficientNet-Deepfake-Detection\",\n",
        "      job_type=\"training\",\n",
        "      config={\n",
        "      \"learning_rate\": LEARNING_RATE,\n",
        "      \"batch_size\": BATCH_SIZE,\n",
        "      \"epochs\": NUM_EPOCHS,\n",
        "      \"architecture\": \"EfficientNetV2_s\",\n",
        "      \"dataset\": \"DFFD distilled\"\n",
        "      })\n",
        "# Ciclo di addestramento\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        wandb.log({\"training_loss\": running_loss})\n",
        "\n",
        "        if (i + 1) % 100 == 0:  # Print progress every 100 batches\n",
        "            print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    train_losses.append(epoch_loss)\n",
        "    print(f'Epoch {epoch+1}/{NUM_EPOCHS} finished, Average Loss: {epoch_loss:.4f}')"
      ],
      "metadata": {
        "id": "koRB2gPp3XmM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "outputId": "6f582b5d-c54d-4c21-e025-de733fa14722"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6ffcaf4540>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1582, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 948, in wait\n",
            "    ready = selector.select(timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-1615052813.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation"
      ],
      "metadata": {
        "id": "W91PChxMsJVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "print(f'Accuratezza sul set di test: {accuracy:.4f}')\n",
        "wandb.log({\"test_accuracy\": accuracy})"
      ],
      "metadata": {
        "id": "jtP5Mq8JsL68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Inference\n"
      ],
      "metadata": {
        "id": "8kVUHnSG3eD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nReport di classificazione:\")\n",
        "report = classification_report(all_labels, all_preds, target_names=['Real', 'Fake'])\n",
        "wandb.log({\"classification_report\": wandb.Html(report)})\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(probs=None, y_true=all_labels, preds=all_preds, class_names=['Real', 'Fake'])})\n",
        "print(report)"
      ],
      "metadata": {
        "id": "9Xr2tgR33i1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting"
      ],
      "metadata": {
        "id": "VGhKUmN-5MLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1, NUM_EPOCHS+1), train_losses, label='Loss di addestramento')\n",
        "plt.xlabel('Epoca')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Andamento della Loss durante l\\'addestramento')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'], cmap='Blues')\n",
        "plt.xlabel('Predetto')\n",
        "plt.ylabel('Reale')\n",
        "plt.title('Matrice di Confusione')\n",
        "plt.show()\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "RQxnbqqV5OIE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}